#!/usr/bin/env python
# -*- mode:python -*-

from __future__ import print_function, with_statement

import sys
import hashlib
import tempfile
import os
import subprocess
import shlex
import shutil
import itertools
import threading
import time
import collections
import unicodedata
import traceback
import weakref
import argparse
import fnmatch
import re
try:
    import Queue # Python 2
except ImportError:
    pass # Python 3 not supported

if not type(sys.version_info) is tuple and sys.version_info.major > 2:
    sys.stderr.write('git-fat does not support Python-3 yet.  Please use python2.\n')
    sys.exit(1)

try:
    from subprocess import check_output
    del check_output
except ImportError:
    def backport_check_output(*popenargs, **kwargs):
        r"""Run command with arguments and return its output as a byte string.

        Backported from Python 2.7 as it's implemented as pure python on stdlib.

        >>> check_output(['/usr/bin/python', '--version'])
        Python 2.6.2
        """
        process = subprocess.Popen(stdout=subprocess.PIPE, *popenargs, **kwargs)
        output, unused_err = process.communicate()
        retcode = process.poll()
        if retcode:
            cmd = kwargs.get("args")
            if cmd is None:
                cmd = popenargs[0]
            error = subprocess.CalledProcessError(retcode, cmd)
            error.output = output
            raise error
        return output
    subprocess.check_output = backport_check_output

BLOCK_SIZE = 4096

class Enum(object):
    def __init__(self, name, **kwargs):
        self.name = name
        for key in kwargs: setattr(self, key, kwargs[key])
    def __str__(self): return self.name

class Style():
    BOLD = '\33[1m'
    BLACK = '\033[30m'
    RED = '\033[31m'
    GREEN = '\033[32m'
    YELLOW = '\033[33m'
    BLUE = '\033[34m'
    MAGENTA = '\033[35m'
    CYAN = '\033[36m'
    WHITE = '\033[37m'
    UNDERLINE = '\033[4m'
    RESET = '\033[0m'

class FatStatus(Enum):
    OK      = Enum('here')
    MISSING = Enum('lost')
    GARBAGE = Enum('junk')
    NOTFAT  = Enum('----')
    byname  = {e.name: e for e in [OK, MISSING, GARBAGE, NOTFAT]}

class FileStatus():
    OK      = Enum('here')
    ORPHAN  = Enum('stub')
    MISSING = Enum('lost')
    DIFF    = Enum('diff')
    NOTFAT  = Enum('thin')
    byname  = {e.name: e for e in [OK, ORPHAN, MISSING, DIFF, NOTFAT]}

class PrintStatus():
    DEFAULT = Enum('default', is_error=False, style=None)
    OK      = Enum('ok',      is_error=False, style=Style.GREEN + Style.BOLD)
    WARN3   = Enum('warn3',   is_error=True,  style=Style.CYAN)
    WARN2   = Enum('warn2',   is_error=True,  style=Style.BLUE)
    WARN    = Enum('warn',    is_error=True,  style=Style.YELLOW + Style.BOLD)
    ERROR   = Enum('error',   is_error=True,  style=Style.RED + Style.BOLD)

def print_status(status, *args, **kwargs):
    if status and status.is_error and not 'file' in kwargs:
        kwargs['file'] = sys.stderr
    if status and status.style and args:
        style = status.style
        file = kwargs['file'] if 'file' in kwargs else None
        end = kwargs.pop('end') if 'end' in kwargs else None
        kwargs['end'] = ''
    else:
        style = None
    if style: print(style, file=file, end='')
    result = print(*args, **kwargs)
    if style: print(Style.RESET, file=file, end=end)
    return result
def verbose_stderr(*args, **kwargs):
    return print(*args, file=sys.stderr, **kwargs)
def verbose_ignore(*args, **kwargs):
    pass
def confirm(message):
    print_status(PrintStatus.OK, '%s. Are you sure? [y,N]: ' % message, end='')
    answer = ''
    try:
        answer = sys.stdin.read(1)
    except:
        pass
    return answer.lower() == 'y'

def mkdir_p(path):
    import errno
    try:
        os.makedirs(path)
    except OSError as exc: # Python >2.5
        if exc.errno == errno.EEXIST and os.path.isdir(path):
            pass
        else: raise

def mkdir_no_clash(name, in_dir='.'):
    import errno
    i = 0
    original_path = path = os.path.join(in_dir, name)
    while True:
        try:
            os.mkdir(path)
            return path
        except OSError as exc:
            if exc.errno == errno.EEXIST and os.path.isdir(path) and i < 1000:
                i += 1
                path = '%s (%i)' % (original_path, i)
            else: raise

def symlink_no_clash(src, dst):
    import errno
    i = 0
    dst_parts = None
    while True:
        try:
            os.symlink(src, dst)
            return dst
        except OSError as exc:
            if exc.errno == errno.EEXIST and i < 1000:
                i += 1
                if not dst_parts: dst_parts = os.path.splitext(dst)
                dst = '%s (%i)%s' % (dst_parts[0], i, dst_parts[1])
            else: raise

def umask():
    """Get umask without changing it."""
    old = os.umask(0)
    os.umask(old)
    return old

def readblocks(stream):
    bytes = 0
    while True:
        data = stream.read(BLOCK_SIZE)
        bytes += len(data)
        if not data:
            break
        yield data
def cat_iter(initer, outstream):
    for block in initer:
        outstream.write(block)
def cat(instream, outstream):
    return cat_iter(readblocks(instream), outstream)
def difftreez_reader(input):
    """Incremental reader for git diff-tree -z output

    :oldmode newmode oldsha1 newsha1 modflag\0filename\0:oldmode newmode ...
    """
    buffer = []
    partial = ''
    while True:
        newread = input.read(BLOCK_SIZE)
        if not newread:
            break
        partial += newread
        while True:
            head, sep, partial = partial.partition('\0')
            if not sep:
                partial = head
                break
            buffer.append(head)
            if len(buffer) == 2:
                oldmode, newmode, oldhash, newhash, modflag = buffer[0].split()
                path = buffer[1]
                yield (newhash, modflag, path)
                buffer = []
def gitconfig_get(name, file=None):
    args = ['git', 'config', '--get']
    if file is not None:
        args += ['--file', file]
    args.append(name)
    p = subprocess.Popen(args, stdout=subprocess.PIPE)
    output = p.communicate()[0].strip()
    if p.returncode and file is None:
        return None
    elif p.returncode:
        return gitconfig_get(name)
    else:
        return output
def gitconfig_set(name, value, file=None):
    args = ['git', 'config']
    if file is not None:
        args += ['--file', file]
    args += [name, value]
    p = subprocess.check_call(args)

class NewlineHelpFormatter(argparse.HelpFormatter):
    '''
    Preserve newlines in argparse.add_argument help text
    '''
    def _split_lines(self, text, width):
        if '\n' in text:
            return text.splitlines()
        return argparse.HelpFormatter._split_lines(self, text, width)

class ThreadWithResult(threading.Thread):
    def __init__(self, *args, **kwargs):
        if 'target' in kwargs:
            target = kwargs.pop('target')
            weak_self = weakref.ref(self)
            def function(*args, **kwargs):
                if weak_self():
                    result = target(*args, **kwargs)
                    strong_self = weak_self()
                    if strong_self: strong_self.result = result
            kwargs['target'] = function
        super(ThreadWithResult, self).__init__(*args, **kwargs)

class Pipeline(object):
    class Popen(object):
        def __init__(self, command):
            self.command = command.split() if isinstance(command, str) else command
            self.process = None
        def pipeline_will_start(self, prev): pass
        def start(self, prev, next):
            if prev:
                if prev.stdout:
                    stdin = prev.stdout
                else:
                    stdin = subprocess.PIPE
            else:
                stdin = None
            self.process = subprocess.Popen(self.command, stdin=stdin, stdout=subprocess.PIPE)
        def wait(self):
            if self.process: self.process.wait()
        def iter_with_prev(self, prev):
            if self.process:
                for line in self.process.stdout:
                    yield line
        def read_with_prev(self, prev, *args):
            if self.process: return self.process.stdout.read(*args)
        def readline_with_prev(self, prev, *args):
            if self.process: return self.process.stdout.readline(*args)
        def should_start_before_next(self, next): return True
        def should_start_after_next(self, next): return False
        uses_pipes = property(lambda self : True)
        error = property(lambda self : None)
        stdin = property(lambda self : (self.process.stdin if self.process else None))
        stdout = property(lambda self : (self.process.stdout if self.process else None))
        def __str__(self): return ' '.join(self.command)

    class IteratorReader(object):
        def __init__(self, iterator):
            self.iterator = iterator
            self.remaining = ''
        def read(self, *args): return self._read(False, *args)
        def readline(self, *args): return self._read(True, *args)
        def readall(self):
            result = self.remaining + '\n'.join(self.iterator)
            self.remaining = ''
            return result
        def _read(self, oneline, *args):
            size = args[0] if args else -1
            if size == 0: return ''
            if size < 0: return self.readall()
            result = self.remaining
            while len(result) < size:
                data = self.iterator.__next__()
                if data:
                    result += data + '\n'
                    if oneline: break
                else: break
            if len(result) >= size:
                self.remaining = result[size:]
                return result[:size]
            else:
                self.remaining = ''
                return result

    class FilterOutput(object):
        def __init__(self, target):
            self.target = target
            self.thread = None
            self.reader = None
            self.error = None
        def pipeline_will_start(self, prev): self.target.pipeline_will_start(prev)
        def start(self, prev, next):
            if next and next.stdin:
                stdin = prev.stdout or prev
                stdout = next.stdin
                self.thread = threading.Thread(target=self.pipe_into, args=(stdin, stdout))
                self.thread.daemon = True
                self.thread.start()
        def pipe_into(self, stdin, stdout):
            try:
                for output in self.target.invoke(stdin):
                    if output is not None:
                        stdout.write(output)
            except IOError:
                self.error = traceback.format_exc()
            finally:
                stdout.close()
        def wait(self):
            if self.thread: self.thread.join()
        def iter_with_prev(self, prev):
            for line in self.target.invoke(prev.stdout or prev):
                if isinstance(line, str): line = line.rstrip('\r\n')
                yield line
        def read_with_prev(self, prev, *args):
            if not self.reader: self.reader = Pipeline.IteratorReader(self.target.invoke(prev))
            return self.reader.read(*args)
        def readline_with_prev(self, prev, *args):
            if not self.reader: self.reader = Pipeline.IteratorReader(self.target.invoke(prev))
            return self.reader.readline(*args)
        def should_start_before_next(self, next): return next and not next.uses_pipes
        def should_start_after_next(self, next): return next and next.uses_pipes
        uses_pipes = property(lambda self : False)
        stdin = property(lambda self : None)
        stdout = property(lambda self : None)
        def __str__(self): return str(self.target)

    class FilterTarget(object):
        def __init__(self, target, expects_cache_arg):
            self.target = target
            self.cache_arg = [None] if expects_cache_arg else []
        def pipeline_will_start(self, pipeline): pass
        def invoke(self, input): return self.target(input, *self.cache_arg)
        def __str__(self): return self.target.__name__

    class CachingTarget(object):
        '''
        Whenever you need to call subprocess.Popen, you have to write a string to the process's
        stdin and retrieve a string from its stdout.
        But often you have other data that you would like to 'remember', but which is not
        accepted / understood by the process's input.
        This class allows you to put objects in a cache such that they bypass the process
        and are restored on the other side (implemented using queues).
        '''
        def __init__(self, target, parent):
            self.target = target
            self.inqueue = self.outqueue = None
            self.data = None
        def pipeline_will_start(self, prev):
            if prev:
                for pipeline in prev.pipelines:
                    plugin = pipeline.plugin
                    if isinstance(plugin, Pipeline.FilterOutput) and isinstance(plugin.target, Pipeline.CachingTarget):
                        plugin.target.outqueue = self.inqueue = Queue.Queue()
                        break
        def get(self): return self.data
        def set(self, data): self.data = data
        def invoke(self, input):
            for output in self.iterate(input):
                if self.outqueue:
                    if output or self.data:
                        self.outqueue.put((output, self.data))
                    if output:
                        yield output
                else:
                    if self.data:
                        yield self.data
            if self.outqueue: self.outqueue.put(None)
        def iterate(self, input):
            if self.inqueue:
                iter_target = None
                while True:
                    record = self.inqueue.get(True, 60)
                    if not record: break # EOF
                    self.data = record[1]
                    output = None
                    if record[0]:
                        if not iter_target:
                            iter_target = self.target(input, self)
                        output = iter_target.next()
                    yield output
            else:
                for output in self.target(input, self): yield output
        def __str__(self): return self.target.__name__

    def __init__(self, plugin, parent):
        self.plugin = plugin
        self.parent = parent
    @staticmethod
    def Process(command, parent=None):
        return Pipeline(Pipeline.Popen(command), parent)
    @staticmethod
    def Filter(target, parent=None, use_cache=None):
        target = Pipeline.CachingTarget(target, parent) if use_cache else Pipeline.FilterTarget(target, use_cache is not None)
        return Pipeline(Pipeline.FilterOutput(target), parent)
    def pipe_into(self, command):
        return Pipeline.Process(command, self)
    def pipe_into_pipeline(self, pipeline):
        pipeline.root.parent = self
        return pipeline
    def filter_output(self, target, use_cache=None):
        return Pipeline.Filter(target, self, use_cache) if target else self
    def will_start_next(self, next):
        if self.plugin.should_start_before_next(next): self.start(next)
    def did_start_next(self, next):
        if self.plugin.should_start_after_next(next): self.start(next)
    def start(self, next=None):
        if not next:
            for pipeline in self.pipelines: pipeline.plugin.pipeline_will_start(pipeline.parent)
        if self.parent: self.parent.will_start_next(self)
        self.plugin.start(self.parent, next)
        if self.parent: self.parent.did_start_next(self)
        return self
    def wait(self):
        for pipeline in self.pipelines: pipeline.plugin.wait()
        return self
    def get_root(self):
        for pipeline in self.pipelines: root = pipeline
        return root
    def get_pipelines(self):
        pipeline = self
        while pipeline:
            yield pipeline
            pipeline = pipeline.parent
    root = property(lambda self : self.get_root())
    pipelines = property(lambda self : self.get_pipelines())
    uses_pipes = property(lambda self : self.plugin.uses_pipes)
    stdin = property(lambda self : self.plugin.stdin)
    stdout = property(lambda self : self.plugin.stdout)
    error = property(lambda self : next((pipeline.plugin.error for pipeline in self.pipelines if pipeline.plugin.error), None))
    def __str__(self): return '\n'.join(reversed([str(pipeline.plugin) for pipeline in self.pipelines]))
    def __iter__(self): return self.plugin.iter_with_prev(self.parent)
    def read(self, *args): return self.plugin.read_with_prev(self.parent, *args)
    def readline(self, *args): return self.plugin.readline_with_prev(self.parent, *args)

class GitFat(object):
    DecodeError = RuntimeError
    def __init__(self, verbose=False):
        self.verbose = verbose_stderr if verbose else verbose_ignore
        try:
            self.gitroot = subprocess.check_output('git rev-parse --show-toplevel'.split()).strip()
        except subprocess.CalledProcessError:
            sys.exit(1)
        self.gitdir = subprocess.check_output('git rev-parse --git-dir'.split()).strip()
        self.objdir = os.path.join(self.gitdir, 'fat', 'objects')
        if os.environ.get('GIT_FAT_VERSION') == '1':
            self.encode = self.encode_v1
        else:
            self.encode = self.encode_v2
        def magiclen(enc):
            return len(enc(hashlib.sha1('dummy').hexdigest(), 5))
        self.magiclen = magiclen(self.encode) # Current version
        self.magiclens = [magiclen(enc) for enc in [self.encode_v1, self.encode_v2]] # All prior versions
    def setup(self):
        mkdir_p(self.objdir)
    def is_init_done(self):
        return gitconfig_get('filter.fat.clean') or gitconfig_get('filter.fat.smudge')
    def is_init_done_v2(self):
        return self.is_init_done() and '%f' in gitconfig_get('filter.fat.clean')
    def assert_init_done(self):
        if not self.is_init_done():
            sys.stderr.write('fatal: git-fat is not yet configured in this repository.\n')
            sys.stderr.write('Run "git fat init" to configure.\n')
            sys.exit(1)
    def get_rsync(self):
        cfgpath   = os.path.join(self.gitroot,'.gitfat')
        remote    = gitconfig_get('rsync.remote', file=cfgpath)
        ssh_port  = gitconfig_get('rsync.sshport', file=cfgpath)
        ssh_user  = gitconfig_get('rsync.sshuser', file=cfgpath)
        options   = gitconfig_get('rsync.options', file=cfgpath)
        if remote is None:
            raise RuntimeError('No rsync.remote in %s' % cfgpath)
        return remote, ssh_port, ssh_user, options
    def get_rsync_command(self,push,list_only=False):
        (remote, ssh_port, ssh_user, options) = self.get_rsync()
        if list_only:
            self.verbose('Querying %s' % (remote))
        elif push:
            self.verbose('Pushing to %s' % (remote))
        else:
            self.verbose('Pulling from %s' % (remote))

        cmd = ['rsync', '--from0', '--files-from=-']
        if list_only:
            cmd.append('--list-only')
#             cmd += ['-nr','--out-format=\'\%n\'','--dry-run']
        else:
            cmd += ['--progress', '--ignore-existing']
        rshopts = ''
        if ssh_user:
            rshopts += ' -l ' + ssh_user
        if ssh_port:
            rshopts += ' -p ' + ssh_port
        if rshopts:
            cmd.append('--rsh=ssh' + rshopts)
        if options:
            cmd += options.split(' ')
        if push:
            cmd += [self.objdir + '/', remote + '/']
        else:
            cmd += [remote + '/', self.objdir + '/']
        return cmd
    def file_hash(self, fname):
        try:
            h = hashlib.new('sha1')
            for block in readblocks(open(fname)):
                h.update(block)
            return h.hexdigest()
        except OSError:
            return None
    def revparse(self, revname):
        return subprocess.check_output(['git', 'rev-parse', revname]).strip()
    def encode_v1(self, digest, bytes):
        'Produce legacy representation of file to be stored in repository.'
        return '#$# git-fat %s\n' % (digest,)
    def encode_v2(self, digest, bytes):
        'Produce representation of file to be stored in repository. 20 characters can hold 64-bit integers.'
        return '#$# git-fat %s %20d\n' % (digest, bytes)
    def decode(self, string, noraise=False):
        cookie = '#$# git-fat '
        if string.startswith(cookie):
            parts = string[len(cookie):].split()
            digest = parts[0]
            bytes = int(parts[1]) if len(parts) > 1 else None
            return digest, bytes
        elif noraise:
            return None, None
        else:
            raise GitFat.DecodeError('Could not decode %s' % (string))
    def decode_stream(self, stream):
        'Return digest if git-fat cache, otherwise return iterator over entire file contents'
        preamble = stream.read(self.magiclen)
        try:
            return self.decode(preamble)
        except GitFat.DecodeError:
            # Not sure if this is the right behavior
            return itertools.chain([preamble], readblocks(stream)), None
    def decode_file(self, fname):
        # Fast check
        try:
            stat = os.lstat(fname)
        except OSError:
            return False, None, 0
        filesize = stat.st_size
        if filesize != self.magiclen:
            return False, None, filesize
        # read file
        try:
            digest, bytes = self.decode_stream(open(fname))
        except IOError:
            return False, None, filesize
        if isinstance(digest, str):
            return digest, bytes, filesize
        else:
            return None, bytes, filesize
    def decode_index_file(self, filename):
        filesize = 0
        metadata = subprocess.check_output(['git', 'ls-files', '-s', filename])
        if metadata:
            objhash = metadata.split(None, 2)[1]
            filesize = int(subprocess.check_output(['git', 'cat-file', '-s', objhash]))
            if filesize == self.magiclen:
                content = subprocess.check_output(['git', 'cat-file', '-p', objhash])
                try:
                    digest, bytes = self.decode(content)
                    return digest, bytes, filesize
                except GitFat.DecodeError:
                    pass
        return False, None, filesize
    def decode_clean(self, body):
        '''
        Attempt to decode version in working tree. The tree version could be changed to have a more
        useful message than the machine-readable copy that goes into the repository. If the tree
        version decodes successfully, it indicates that the fat data is not currently available in
        this repository.
        '''
        digest, bytes = self.decode(body, noraise=True)
        return digest
    def filter_clean(self, instream, outstreamclean, filename=None):
        h = hashlib.new('sha1')
        bytes = 0
        fd, tmpname = tempfile.mkstemp(dir=self.objdir)
        suffix_for_verbose = ' %s' % filename if filename else ''
        try:
            ishanging = False
            cached = False                # changes to True when file is cached
            with os.fdopen(fd, 'w') as cache:
                outstream = cache
                firstblock = True
                for block in readblocks(instream):
                    if firstblock:
                        if len(block) == self.magiclen and self.decode_clean(block[0:self.magiclen]):
                            ishanging = True              # Working tree version is verbatim from repository (not smudged)
                            outstream = outstreamclean
                        elif filename:
                            # If previously added without using fat, don't try to convert it to fat now
                            digest, _, filesize = self.decode_index_file(filename)
                            if not digest and filesize > 0:
                                self.verbose('git-fat filter-clean: skipping because previously added as non-fat file: %s' % filename)
                                ishanging = True
                                outstream = outstreamclean
                        firstblock = False
                    h.update(block)
                    bytes += len(block)
                    outstream.write(block)
                outstream.flush()
            digest = h.hexdigest()
            objfile = os.path.join(self.objdir, digest)
            if not ishanging:
                if os.path.exists(objfile):
                    self.verbose('git-fat filter-clean: cache already exists %s%s' % (objfile, suffix_for_verbose))
                    os.remove(tmpname)
                else:
                    # Set permissions for the new file using the current umask
                    os.chmod(tmpname, int('444', 8) & ~umask())
                    os.rename(tmpname, objfile)
                    self.verbose('git-fat filter-clean: caching to %s%s' % (objfile, suffix_for_verbose))
                cached = True
                outstreamclean.write(self.encode(digest, bytes))
        finally:
            if not cached:
                os.remove(tmpname)

    def cmd_filter_clean(self, filename=None):
        '''
        The clean filter runs when a file is added to the index. It gets the "smudged" (tree)
        version of the file on stdin and produces the "clean" (repository) version on stdout.
        '''
        self.setup()
        self.filter_clean(sys.stdin, sys.stdout, filename)
    def cmd_filter_smudge(self, filename=None):
        self.setup()
        result, bytes = self.decode_stream(sys.stdin)
        suffix_for_verbose = ' %s' % filename if filename else ''
        if isinstance(result, str):       # We got a digest
            objfile = os.path.join(self.objdir, result)
            try:
                cat(open(objfile), sys.stdout)
                self.verbose('git-fat filter-smudge: restoring from %s%s' % (objfile, suffix_for_verbose))
            except IOError:                 # file not found
                self.verbose('git-fat filter-smudge: fat object missing %s%s' % (objfile, suffix_for_verbose))
                sys.stdout.write(self.encode(result, bytes))   # could leave a better notice about how to recover this file
        else:                             # We have an iterable over the original input.
            self.verbose('git-fat filter-smudge: not a managed file%s' % suffix_for_verbose)
            cat_iter(result, sys.stdout)
    def catalog_objects(self):
        try:
            return set(os.listdir(self.objdir)) # Dir may not exist
        except OSError:
            return set()
    def referenced_objects(self, rev=None, all=False):
        get_referenced = self.fathashes_pipeline(rev, all).start()
        result = {x for x in get_referenced}
        get_referenced.wait()
        if get_referenced.error: self.verbose(get_referenced.error)
        return result
    def fathashes_pipeline(self, rev=None, all=False):
        if all:
            rev = '--all'
        elif rev is None:
            rev = self.revparse('HEAD')
        return Pipeline.Process(['git','rev-list','--objects',rev]) \
            .pipe_into_pipeline(self.fat_for_objhash_pipeline(only_fathashes=True))
    def fatobjects_pipeline(self):
        return Pipeline.Process(['git','rev-list','--objects','--all']) \
            .pipe_into_pipeline(self.fat_for_objhash_pipeline(only_fatobjects=True))

    class FatObject(object):
        def __init__(self):
            self.fathash = self.fatsize = self.objhash = self.objsize = self.filename = \
            self.fat_is_filter = self.filestatus = self.fatstatus = None
        def __str__(self):
            return '%s\t%s\t%s\t%s\t%s' % (self.fathash, self.fatsize, self.objhash, self.objsize, self.filename)
        def __hash__(self): return self.fathash.__hash__()
        def __cmp__(self, other): return self.fathash.__cmp__(other)
        def __eq__(self, other): return self.fathash.__eq__(other)

    def fat_for_objhash_pipeline(self, only_fathashes=False, only_fatobjects=False):
        def cut_objhash(input, cache):
            for line in input:
                parts = line.split(None, 1) # objhash, filename_with_spaces_and_newline
                objhash = parts[0]
                if cache:
                    fatobj = GitFat.FatObject()
                    fatobj.objhash = objhash
                    if len(parts) > 1: fatobj.filename = parts[1].rstrip()
                    cache.set(fatobj)
                yield objhash + '\n'
        def filter_gitfat_candidates(input, cache):
            for line in input:
                output = None
                parts = line.split() # objhash, objtype, size, \n
                if len(parts) >= 3:
                    objhash, objtype, size_str = parts
                    size = int(size_str)
                    if cache: cache.get().objsize = size
                    if objtype == 'blob' and size in self.magiclens:
                        output = objhash + '\n'
                if not output and (only_fathashes or only_fatobjects):
                    if cache: cache.set(None)
                yield output
        def fathash_from_file_contents(input, cache):
            # Process metadata + content format provided by `cat-file --batch`
            while True:
                metadata_line = input.readline()
                if not metadata_line:
                    break  # EOF
                objhash, objtype, size_str = metadata_line.split()
                size, bytes_read = int(size_str), 0
                # We know from filter that item is a candidate git-fat object and
                # is small enough to read into memory and process
                content = ''
                while bytes_read < size:
                    data = input.read(size - bytes_read)
                    if not data:
                        break  # EOF
                    content += data
                    bytes_read += len(data)
                try:
                    fathash, fatsize = self.decode(content)
                    if cache:
                        fatobj = cache.get()
                        fatobj.fathash = fathash
                        fatobj.fatsize = fatsize
                    yield fathash
                except GitFat.DecodeError:
                    self.verbose('decode error: %s' % objhash)
                    yield None
                # Consume LF record delimiter in `cat-file --batch` output
                bytes_read = 0
                while bytes_read < 1:
                    data = input.read(1)
                    if not data:
                        break  # EOF
                    bytes_read += len(data)

        # 1. Cut hash from revlist input
        # 2. Get info for git-fat candidates in bulk
        # 3. Analyse info to identify git-fat candidates
        # 4. Get full contents of identified git-fat candidates in bulk
        # 5. Extract fathash using file metadata and contents
        use_cache = not only_fathashes
        return Pipeline.Filter(cut_objhash, use_cache=use_cache) \
            .pipe_into('git cat-file --batch-check') \
            .filter_output(filter_gitfat_candidates, use_cache=use_cache) \
            .pipe_into('git cat-file --batch') \
            .filter_output(fathash_from_file_contents, use_cache=use_cache)

    def fat_files_pipeline(self, paths=['.']):
        def having_fat_filter(input):
            # Input format = [mode] [obj] [stage] [filename] \x00 filter \x00 fat \x00
            part_index = 0
            filename = obj = None
            for line in input:
                for part in line.split('\x00'):
                    if not part:
                        break; # EOF
                    if part_index == 0:
                        mode, obj, stage, filename = part.split(None, 3)
                        if mode.startswith('100'):
                            filename = filename.rstrip()
                        else:
                            filename = obj = None
                    elif part_index == 1:
                        if filename and part != 'filter':
                            filename = obj = None
                    elif part_index == 2:
                        if filename and part == 'fat':
                            yield '%s %s\n' % (obj, filename)
                    part_index = (part_index + 1) % 3
        def add_status(input):
            for fatobj in input:
                self.update_fatobj_status(fatobj)
                yield fatobj

        paths = [os.path.relpath(os.path.abspath(path), '.') for path in paths]
        return Pipeline.Process(['git','ls-files','-s','-z'] + list(paths)) \
            .pipe_into('git check-attr --stdin -z filter') \
            .filter_output(having_fat_filter) \
            .pipe_into_pipeline(self.fat_for_objhash_pipeline()) \
            .filter_output(add_status)

    def print_fatobj(self, fatobj):
        if fatobj.fatstatus == FatStatus.NOTFAT:
            printstatus = PrintStatus.WARN3
        elif fatobj.fatstatus == FatStatus.GARBAGE:
            printstatus = PrintStatus.WARN2
        elif fatobj.fatstatus == FatStatus.MISSING:
            if fatobj.filestatus == FileStatus.DIFF or fatobj.filestatus == FileStatus.MISSING:
                printstatus = PrintStatus.ERROR
            else:
                printstatus = PrintStatus.WARN
        elif fatobj.filestatus == FileStatus.ORPHAN:
            if fatobj.fatstatus == FatStatus.MISSING:
                printstatus = PrintStatus.ERROR
            else:
                printstatus = PrintStatus.WARN
        else:
            printstatus = PrintStatus.DEFAULT
        status = fatobj.status or OK_STATUS
        print_status(printstatus, '%s %s %s' % (status, fatobj.fathash or fatobj.objhash, fatobj.filename))

    def update_fatobj_status(self, fatobj, catalog=None):
        if fatobj.fathash:
            if catalog:
                fatstatus = FileStatus.OK if fatobj.fathash in catalog else FatStatus.MISSING
            else:
                fatfile = os.path.join(self.objdir, fatobj.fathash)
                fatstatus = FatStatus.OK if os.path.exists(fatfile) else FatStatus.MISSING
            if not fatobj.filename:
                filestatus = FileStatus.MISSING
            else:
                fat_from_file, fatsize_from_file, filesize = self.decode_file(fatobj.filename)
                if fat_from_file:
                    filestatus = FileStatus.ORPHAN
                else:
                    if filesize == 0:
                        filestatus = FileStatus.MISSING
                    elif filesize != fatobj.fatsize:
                        filestatus = FileStatus.DIFF
                    else:
                        filestatus = FileStatus.OK
        else:
            filestatus = FileStatus.NOTFAT
            if not fatobj.filename:
                filestatus = FileStatus.MISSING
            else:
                filehash = self.file_hash(fatobj.filename)
                if filehash and os.path.exists(os.path.join(self.objdir, fatobj.objhash)):
                    fatstatus = FatStatus.GARBAGE
                else:
                    fatstatus = FatStatus.NOTFAT
        status = self.encode_status(filestatus, fatstatus)
        fatobj.fatstatus = fatstatus
        fatobj.filestatus = filestatus
        fatobj.status = status
        return fatobj
    def fatobj_status_predicate(self, filestatus=None, fatstatus=None):
        def make_fatobj_predicate(pattern, attrib_getter):
            if pattern:
                regex = re.compile(fnmatch.translate('[%s]*' % pattern))
                return lambda obj : True if regex.match(attrib_getter(obj).name) else False
            else: return None
        def and_predicate(a, b):
            if a and b: return lambda x : a(x) and b(x)
            else: return a or b
        filestatus_predicate = make_fatobj_predicate(filestatus, lambda obj : obj.filestatus)
        fatstatus_predicate = make_fatobj_predicate(fatstatus, lambda obj : obj.fatstatus)
        return and_predicate(filestatus_predicate, fatstatus_predicate)

    OK_STATUS = '-' * 11
    def encode_status(self, filestatus, fatstatus):
        if filestatus == FileStatus.OK and fatstatus == FatStatus.OK:
            return GitFat.OK_STATUS
        else:
            return 'x-%s-%s' % (filestatus, fatstatus)
    def decode_status(self, status):
        if status == GitFat.OK_STATUS:
            return (FileStatus.OK, FatStatus.OK)
        else:
            filestatus = FileStatus.byname[status[2:6]] or FileStatus.OK
            fatstatus = FatStatus.byname[status[7:11]] or FatStatus.OK
            return (filestatus, fatstatus)

    def orphan_files(self, patterns=[]):
        'generator for all orphan placeholders in the working tree'
        if not patterns or patterns == ['']:
            patterns = ['.']
        for fname in subprocess.check_output(['git', 'ls-files', '-z'] + patterns).split('\x00')[:-1]:
            digest = self.decode_file(fname)[0]
            if digest:
                yield (digest, fname)

    def filestatus_arg_help(self): return 'Include only objects whose file status has any of the specified characters as its first letter\n\nFILESTATUS: %s' % ', '.join('[%s]%s' % (x[0], x[1:]) for x in FileStatus.byname.keys())
    def fatstatus_arg_help(self):  return 'Include only objects whose fat status has any of the specified characters as its first letter\n\nFATSTATUS: %s' % ', '.join('[%s]%s' % (x[0], x[1:]) for x in FatStatus.byname.keys())

    def cmd_status(self, args):
        parser = argparse.ArgumentParser(prog='git fat status', formatter_class=NewlineHelpFormatter)
        parser.add_argument('--all', action='store_true', help='Show all fat objects ever committed to this repository')
        parser.add_argument('--filestatus', help=self.filestatus_arg_help())
        parser.add_argument('--fatstatus', help=self.fatstatus_arg_help())
        parser.add_argument('paths', nargs=argparse.REMAINDER, help='Show status for only the specified paths in the working tree.\nIf a directory path is specified, all nested files and subdirectories are searched.')
        arg = parser.parse_args(args)
        fatobj_predicate = self.fatobj_status_predicate(arg.filestatus, arg.fatstatus)
        if arg.paths:
            self.status_for_paths(arg.paths, fatobj_predicate=fatobj_predicate)
        else:
            self.status_no_paths(all=arg.all, fatobj_predicate=fatobj_predicate)
    def status_no_paths(self, all=False, fatobj_predicate=None):
        load_referenced = self.fatobjects_pipeline().start()
        catalog = self.catalog_objects()
        if all or fatobj_predicate:
            # Ungrouped - print objects as arrive from pipe
            for fatobj in load_referenced:
                self.update_fatobj_status(fatobj, catalog=catalog)
                if not fatobj_predicate or fatobj_predicate(fatobj): self.print_fatobj(fatobj)
            load_referenced.wait()
            if load_referenced.error: self.verbose(load_referenced.error)
        else:
            # Grouped - load all objects into memory first
            referenced = {obj for obj in load_referenced}
            load_referenced.wait()
            if load_referenced.error: self.verbose(load_referenced.error)
            orphans = referenced - catalog
            garbages = catalog - referenced
            if orphans:
                print_status(PrintStatus.WARN, 'Orphan objects:')
                for orphan in orphans:
                    self.update_fatobj_status(orphan, catalog=catalog)
                    self.print_fatobj(orphan)
            if garbages:
                print_status(PrintStatus.WARN2, 'Garbage objects:')
                indent = ' ' * 12
                for garbage in garbages:
                    print_status(PrintStatus.WARN2, '%s%s' % (indent, garbage))
    def status_for_paths(self, paths, fatobj_predicate=None):
        # Ungrouped - print objects as arrive from pipe
        get_objects = self.fat_files_pipeline(paths).start()
        for fatobj in get_objects:
            if not fatobj_predicate or fatobj_predicate(fatobj): self.print_fatobj(fatobj)
        get_objects.wait()
        if get_objects.error: self.verbose(get_objects.error)
    def is_dirty(self):
        return subprocess.call(['git', 'diff-index', '--quiet', 'HEAD']) == 0
    def cmd_push(self, args):
        'Push anything that I have stored and referenced'
        self.setup()
        # Default to push only those objects referenced by current HEAD
        # (includes history). Finer-grained pushing would be useful.
        pushall = '--all' in args
        files = self.referenced_objects(all=pushall) & self.catalog_objects()
        cmd = self.get_rsync_command(push=True)
        self.verbose('Executing: %s' % ' '.join(cmd))
        p = subprocess.Popen(cmd, stdin=subprocess.PIPE)
        p.communicate(input='\x00'.join(files))
        if p.returncode:
            sys.exit(p.returncode)
    def checkout(self, files=None, pull_if_needed=False):
        'Update any stale files in the present working tree'
        self.assert_init_done()
        orphan_files = self.orphan_files(files)
        if pull_if_needed:
            orphan_files = list(orphan_files)
            orphan = set([orphan_file[0] for orphan_file in orphan_files])
            catalog = self.catalog_objects()
            missing = orphan - catalog
            if missing:
                self.pull(missing)
        for digest, fname in orphan_files:
            objpath = os.path.join(self.objdir, digest)
            if os.access(objpath, os.R_OK):
                print('Restoring %s -> %s' % (digest, fname))
                # The output of our smudge filter depends on the existence of
                # the file in .git/fat/objects, but git caches the file stat
                # from the previous time the file was smudged, therefore it
                # won't try to re-smudge. I don't know a git command that
                # specifically invalidates that cache, but changing the mtime
                # on the file will invalidate the cache.
                # Here we set the mtime to mtime + 1. This is an improvement
                # over touching the file as it catches the edgecase where a
                # git-checkout happens within the same second as a git fat
                # checkout.
                stat = os.lstat(fname)
                os.utime(fname, (stat.st_atime, stat.st_mtime + 1))
                # This re-smudge is essentially a copy that restores
                # permissions.
                subprocess.check_call(
                    ['git', 'checkout-index', '--index', '--force', fname])
            elif pull_if_needed:
                print('Data unavailable: %s %s' % (digest,fname))
    def cmd_pull(self, args):
        'Pull anything that I have referenced, but not stored'
        self.setup()
        refargs = dict()
        if '--all' in args:
            refargs['all'] = True
        for arg in args:
            if arg.startswith('-') or len(arg) != 40:
                continue
            rev = self.revparse(arg)
            if rev:
                refargs['rev'] = rev
        patterns = self.parse_pull_patterns(args)
        files = self.filter_objects(refargs, patterns or [''])
        if files or not patterns:
            self.pull(files)
            self.checkout(files)
    def parse_pull_patterns(self, args):
        if '--' in args:
            idx = args.index('--')
            patterns  = args[idx+1:] #we don't care about '--'
            return patterns
    def pull(self, files):
        cmd = self.get_rsync_command(push=False)
        self.verbose('Executing: %s' % ' '.join(cmd))
        p = subprocess.Popen(cmd, stdin=subprocess.PIPE)
        p.communicate(input='\x00'.join(files))
        if p.returncode:
            sys.exit(p.returncode)

    def filter_objects(self, refargs, patterns):
        files = self.referenced_objects(**refargs) - self.catalog_objects()
        if refargs.get('all'): # Currently ignores patterns; can we efficiently do both?
            return files
        orphans_matched = list(self.orphan_files(patterns))
        orphans_objects = set(map(lambda x: x[0], orphans_matched))
        return files & orphans_objects

    def cmd_checkout(self, args):
        self.checkout(pull_if_needed=True)

    def cmd_uncache(self, args):
        get_objects = self.fat_files_pipeline(args).start()
        candidates = {obj.fathash:obj for obj in get_objects if obj.filestatus == FileStatus.OK and obj.fatstatus == FatStatus.OK}
        get_objects.wait()
        if get_objects.error: self.verbose(get_objects.error)

        count = len(candidates)
        if count == 0:
            print_status(PrintStatus.ERROR, 'No eligible fat files to uncache')
        else:
            plural = 's' if count > 1 else ''
            self.verbose('%i candidate cache file%s found. Checking they exist on remote...' % (count, plural))
            exists_on_remote = set()
            cmd = self.get_rsync_command(push=False, list_only=True)
            self.verbose('Executing: %s' % ' '.join(cmd))
            p = subprocess.Popen(cmd, stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=open('/dev/null', 'w'))
            p.stdin.write('\x00'.join(candidates.keys()))
            p.stdin.close()
            for line in p.stdout:
                parts = line.rstrip().rsplit(None, 1)
                if len(parts) > 1:
                    fathash = parts[1]
                    if fathash:
                        found_candidate = candidates[fathash]
                        self.print_fatobj(found_candidate)
                        exists_on_remote.add(found_candidate)
            p.wait()

            candidates = exists_on_remote
            count = len(candidates)
            if count == 0:
                print_status(PrintStatus.ERROR, 'No eligible fat files to uncache')
            else:
                plural = 's' if count > 1 else ''
                if confirm('%i cache file%s will be deleted' % (count, plural)):
                    for fatobj in candidates:
                        os.remove(os.path.join(self.objdir, fatobj.fathash))
                    self.status_for_paths([fatobj.filename for fatobj in candidates])
                else:
                    print_status(PrintStatus.DEFAULT, 'No changes performed')

    def cmd_dump(self, args):
        parser = argparse.ArgumentParser(prog='git fat dump', formatter_class=NewlineHelpFormatter)
        parser.add_argument('--filestatus', help=self.filestatus_arg_help())
        arg = parser.parse_args(args)
        fatobj_predicate = self.fatobj_status_predicate(filestatus=arg.filestatus)

        if not os.path.exists(self.objdir): return
        rootdir = os.path.normpath(mkdir_no_clash(os.path.join(self.gitdir, 'fat', 'dump')))
        rootdir_to_objdir = os.path.join('..', 'objects')
        print_status(PrintStatus.OK, 'Dumping to %s ...' % rootdir)

        count = 0
        dumpdir_to_objdir = dict()
        load_referenced = self.fatobjects_pipeline().start()
        catalog = self.catalog_objects()
        for fatobj in load_referenced:
            if fatobj.filename and fatobj in catalog:
                self.update_fatobj_status(fatobj, catalog=catalog)
                if not fatobj_predicate or fatobj_predicate(fatobj):
                    dirname = os.path.dirname(fatobj.filename)
                    if not dirname in dumpdir_to_objdir:
                        dumpdir_to_objdir[dirname] = os.path.relpath(rootdir_to_objdir, dirname)
                        if dirname:
                            dir_path = os.path.join(rootdir, dirname)
                            self.verbose('Make dir %s' % dir_path)
                            mkdir_p(dir_path)
                    src = os.path.join(dumpdir_to_objdir[dirname], fatobj.fathash)
                    dst = os.path.join(rootdir, fatobj.filename)
                    symlink_no_clash(src, dst)
                    count += 1
        load_referenced.wait()
        print_status(PrintStatus.OK, '%i fat files' % count)
        if count == 0: os.rmdir(rootdir)

    def cmd_gc(self):
        garbage = self.catalog_objects() - self.referenced_objects()
        print('Unreferenced objects to remove: %d' % len(garbage))
        for obj in garbage:
            fname = os.path.join(self.objdir, obj)
            print('%10d %s' % (os.stat(fname).st_size, obj))
            os.remove(fname)

    def cmd_verify(self):
        """Print details of git-fat objects with incorrect data hash"""
        corrupted_objects = []
        for obj in self.catalog_objects():
            fname = os.path.join(self.objdir, obj)
            data_hash = self.file_hash(fname)
            if obj != data_hash:
                corrupted_objects.append((obj, data_hash))
        if corrupted_objects:
            print('Corrupted objects: %d' % len(corrupted_objects))
            for obj, data_hash in corrupted_objects:
                print('%s data hash is %s' % (obj, data_hash))
            sys.exit(1)

    def cmd_init(self):
        self.setup()
        if self.is_init_done_v2():
            print('Git fat already configured, check configuration in .git/config')
        else:
            gitconfig_set('filter.fat.clean', 'git-fat filter-clean %f')
            gitconfig_set('filter.fat.smudge', 'git-fat filter-smudge %f')
            print('Initialized git fat')
    def gen_large_blobs(self, revs, threshsize):
        """Build dict of all blobs"""
        time0 = time.time()
        def hash_only(input, output):
            """The output of git rev-list --objects shows extra info for blobs, subdirectory trees, and tags.
            This truncates to one hash per line.
            """
            for line in input:
                output.write(line[:40] + '\n')
            output.close()
        revlist = subprocess.Popen(['git', 'rev-list', '--all', '--objects'], stdout=subprocess.PIPE, bufsize=-1)
        objcheck = subprocess.Popen(['git', 'cat-file', '--batch-check'], stdin=subprocess.PIPE, stdout=subprocess.PIPE, bufsize=-1)
        hashonly = threading.Thread(target=hash_only, args=(revlist.stdout, objcheck.stdin))
        hashonly.start()
        numblobs = 0; numlarge = 1
        # Build dict with the sizes of all large blobs
        for line in objcheck.stdout:
            objhash, blob, size = line.split()
            if blob != 'blob':
                continue
            size = int(size)
            numblobs += 1
            if size > threshsize:
                numlarge += 1
                yield objhash, size
        revlist.wait()
        objcheck.wait()
        hashonly.join()
        time1 = time.time()
        self.verbose('%d of %d blobs are >= %d bytes [elapsed %.3fs]' % (numlarge, numblobs, threshsize, time1-time0))
    def cmd_find(self, args):
        maxsize = int(args[0])
        blobsizes = dict(self.gen_large_blobs('--all', maxsize))
        time0 = time.time()
        # Find all names assumed by large blobs (those in blobsizes)
        pathsizes = collections.defaultdict(lambda:set())
        difftree = Pipeline.Process(['git', 'rev-list', '--all']) \
            .pipe_into(['git', 'diff-tree', '--root', '--no-renames', '--no-commit-id', '--diff-filter=AMCR', '-r', '--stdin', '-z']) \
            .start()
        for newblob, modflag, path in difftreez_reader(difftree.stdout):
            bsize = blobsizes.get(newblob)
            if bsize:                     # We care about this blob
                pathsizes[path].add(bsize)
        time1 = time.time()
        self.verbose('Found %d paths in %.3f s' % (len(pathsizes), time1-time0))
        maxlen = max(map(len,pathsizes)) if pathsizes else 0
        for path, sizes in sorted(pathsizes.items(), key=lambda ps: max(ps[1]), reverse=True):
            print('%-*s filter=fat -text # %10d %d' % (maxlen, path,max(sizes),len(sizes)))
        difftree.wait()
    def cmd_index_filter(self, args):
        manage_gitattributes = '--manage-gitattributes' in args
        filelist = set(f.strip() for f in open(args[0]).readlines())
        lsfiles = subprocess.Popen(['git', 'ls-files', '-s'], stdout=subprocess.PIPE)
        updateindex = subprocess.Popen(['git', 'update-index', '--index-info'], stdin=subprocess.PIPE)
        for line in lsfiles.stdout:
            mode, sep, tail = line.partition(' ')
            blobhash, sep, tail = tail.partition(' ')
            stageno, sep, tail = tail.partition('\t')
            filename = tail.strip()
            if filename not in filelist:
                continue
            if mode == "120000":
                # skip symbolic links
                continue
            # This file will contain the hash of the cleaned object
            hashfile = os.path.join(self.gitdir, 'fat', 'index-filter', blobhash)
            try:
                cleanedobj = open(hashfile).read().rstrip()
            except IOError:
                catfile = subprocess.Popen(['git', 'cat-file', 'blob', blobhash], stdout=subprocess.PIPE)
                hashobject = subprocess.Popen(['git', 'hash-object', '-w', '--stdin'], stdin=subprocess.PIPE, stdout=subprocess.PIPE)
                def dofilter():
                    self.filter_clean(catfile.stdout, hashobject.stdin, filename)
                    hashobject.stdin.close()
                filterclean = threading.Thread(target=dofilter)
                filterclean.start()
                cleanedobj = hashobject.stdout.read().rstrip()
                catfile.wait()
                hashobject.wait()
                filterclean.join()
                mkdir_p(os.path.dirname(hashfile))
                open(hashfile, 'w').write(cleanedobj + '\n')
            updateindex.stdin.write('%s %s %s\t%s\n' % (mode, cleanedobj, stageno, filename))
        if manage_gitattributes:
            try:
                mode, blobsha1, stageno, filename = subprocess.check_output(['git', 'ls-files', '-s', '.gitattributes']).split()
                gitattributes_lines = subprocess.check_output(['git', 'cat-file', 'blob', blobsha1]).splitlines()
            except ValueError:  # Nothing to unpack, thus no file
                mode, stageno = '100644', '0'
                gitattributes_lines = []
            gitattributes_extra = ['%s filter=fat -text' % line.split()[0] for line in filelist]
            hashobject = subprocess.Popen(['git', 'hash-object', '-w', '--stdin'], stdin=subprocess.PIPE, stdout=subprocess.PIPE)
            stdout, stderr = hashobject.communicate('\n'.join(gitattributes_lines + gitattributes_extra) + '\n')
            updateindex.stdin.write('%s %s %s\t%s\n' % (mode, stdout.strip(), stageno, '.gitattributes'))
        updateindex.stdin.close()
        lsfiles.wait()
        updateindex.wait()

def main(fat, cmd, args):
    if cmd == 'filter-clean':
        fat.cmd_filter_clean(args[0] if args else None)
    elif cmd == 'filter-smudge':
        fat.cmd_filter_smudge(args[0] if args else None)
    elif cmd == 'init':
        fat.cmd_init()
    elif cmd == 'status':
        fat.cmd_status(args)
    elif cmd == 'push':
        fat.cmd_push(args)
    elif cmd == 'pull':
        fat.cmd_pull(args)
    elif cmd == 'gc':
        fat.cmd_gc()
    elif cmd == 'verify':
        fat.cmd_verify()
    elif cmd == 'checkout':
        fat.cmd_checkout(args)
    elif cmd == 'find':
        fat.cmd_find(*args)
    elif cmd == 'index-filter':
        fat.cmd_index_filter(args)
    elif cmd == 'uncache':
        fat.cmd_uncache(args)
    elif cmd == 'dump':
        fat.cmd_dump(args)
    else:
        print('Usage: git fat [init|status|push|pull|gc|verify|checkout|find|index-filter|uncache|dump] [--verbose]', file=sys.stderr)

if __name__ == '__main__':
    cmd, verbose, args = (sys.argv[1], '--verbose' in sys.argv, [arg for arg in sys.argv[2:] if arg != '--verbose']) if len(sys.argv) > 1 else ('', False, [])
    if not verbose:
        verbose = os.environ.get('GIT_FAT_VERBOSE')
    fat = GitFat(verbose=verbose)
    if verbose:
        main(fat, cmd, args)
    else:
        try:
            main(fat, cmd, args)
        except KeyboardInterrupt:
            sys.exit()
        except Exception as e:
            prefix = '[git-fat] ' if cmd == 'filter-clean' or cmd == 'filter-smudge' else ''
            print('%s%s' % (prefix, e.message or 'An error occurred with type %s' % type(e).__name__), file=sys.stderr)
            sys.exit(1)
