#!/usr/bin/env python
# -*- mode:python -*-

from __future__ import print_function, with_statement

import sys
import hashlib
import tempfile
import os
import subprocess
import shlex
import shutil
import itertools
import threading
import time
import collections
import unicodedata
import traceback
import weakref
try:
    import Queue # Python 2
except ImportError:
    pass # Python 3 not supported

if not type(sys.version_info) is tuple and sys.version_info.major > 2:
    sys.stderr.write('git-fat does not support Python-3 yet.  Please use python2.\n')
    sys.exit(1)

try:
    from subprocess import check_output
    del check_output
except ImportError:
    def backport_check_output(*popenargs, **kwargs):
        r"""Run command with arguments and return its output as a byte string.

        Backported from Python 2.7 as it's implemented as pure python on stdlib.

        >>> check_output(['/usr/bin/python', '--version'])
        Python 2.6.2
        """
        process = subprocess.Popen(stdout=subprocess.PIPE, *popenargs, **kwargs)
        output, unused_err = process.communicate()
        retcode = process.poll()
        if retcode:
            cmd = kwargs.get("args")
            if cmd is None:
                cmd = popenargs[0]
            error = subprocess.CalledProcessError(retcode, cmd)
            error.output = output
            raise error
        return output
    subprocess.check_output = backport_check_output

BLOCK_SIZE = 4096

class Enum(object):
    def __init__(self, name, **kwargs):
        self.name = name
        for key in kwargs: setattr(self, key, kwargs[key])
    def __str__(self): return self.name

class Style():
    BOLD = '\33[1m'
    BLACK = '\033[30m'
    RED = '\033[31m'
    GREEN = '\033[32m'
    YELLOW = '\033[33m'
    BLUE = '\033[34m'
    MAGENTA = '\033[35m'
    CYAN = '\033[36m'
    WHITE = '\033[37m'
    UNDERLINE = '\033[4m'
    RESET = '\033[0m'

class FatStatus(Enum):
    OK      = Enum('here')
    MISSING = Enum('lost')
    GARBAGE = Enum('junk')
    NOTFAT  = Enum('')

class FileStatus():
    OK      = Enum('here')
    ORPHAN  = Enum('stub')
    DIFF    = Enum('diff')
    NOTFAT  = Enum('thin')

class PrintStatus():
    DEFAULT = Enum('default', is_error=False, style=None)
    OK      = Enum('ok',      is_error=False, style=Style.GREEN + Style.BOLD)
    WARN3   = Enum('warn3',   is_error=True,  style=Style.CYAN)
    WARN2   = Enum('warn2',   is_error=True,  style=Style.BLUE)
    WARN    = Enum('warn',    is_error=True,  style=Style.YELLOW + Style.BOLD)
    ERROR   = Enum('error',   is_error=True,  style=Style.RED + Style.BOLD)

def print_status(status, *args, **kwargs):
    if status and status.is_error and not 'file' in kwargs:
        kwargs['file'] = sys.stderr
    if status and status.style and args:
        style = status.style
        file = kwargs['file'] if 'file' in kwargs else None
        end = kwargs.pop('end') if 'end' in kwargs else None
        kwargs['end'] = ''
    else:
        style = None
    if style: print(style, file=file, end='')
    result = print(*args, **kwargs)
    if style: print(Style.RESET, file=file, end=end)
    return result
def verbose_stderr(*args, **kwargs):
    return print(*args, file=sys.stderr, **kwargs)
def verbose_ignore(*args, **kwargs):
    pass

def mkdir_p(path):
    import errno
    try:
        os.makedirs(path)
    except OSError as exc: # Python >2.5
        if exc.errno == errno.EEXIST and os.path.isdir(path):
            pass
        else: raise

def umask():
    """Get umask without changing it."""
    old = os.umask(0)
    os.umask(old)
    return old

def readblocks(stream):
    bytes = 0
    while True:
        data = stream.read(BLOCK_SIZE)
        bytes += len(data)
        if not data:
            break
        yield data
def cat_iter(initer, outstream):
    for block in initer:
        outstream.write(block)
def cat(instream, outstream):
    return cat_iter(readblocks(instream), outstream)
def difftreez_reader(input):
    """Incremental reader for git diff-tree -z output

    :oldmode newmode oldsha1 newsha1 modflag\0filename\0:oldmode newmode ...
    """
    buffer = []
    partial = ''
    while True:
        newread = input.read(BLOCK_SIZE)
        if not newread:
            break
        partial += newread
        while True:
            head, sep, partial = partial.partition('\0')
            if not sep:
                partial = head
                break
            buffer.append(head)
            if len(buffer) == 2:
                oldmode, newmode, oldhash, newhash, modflag = buffer[0].split()
                path = buffer[1]
                yield (newhash, modflag, path)
                buffer = []
def gitconfig_get(name, file=None):
    args = ['git', 'config', '--get']
    if file is not None:
        args += ['--file', file]
    args.append(name)
    p = subprocess.Popen(args, stdout=subprocess.PIPE)
    output = p.communicate()[0].strip()
    if p.returncode and file is None:
        return None
    elif p.returncode:
        return gitconfig_get(name)
    else:
        return output
def gitconfig_set(name, value, file=None):
    args = ['git', 'config']
    if file is not None:
        args += ['--file', file]
    args += [name, value]
    p = subprocess.check_call(args)

class ThreadWithResult(threading.Thread):
    def __init__(self, *args, **kwargs):
        if 'target' in kwargs:
            target = kwargs.pop('target')
            weak_self = weakref.ref(self)
            def function(*args, **kwargs):
                if weak_self():
                    result = target(*args, **kwargs)
                    strong_self = weak_self()
                    if strong_self: strong_self.result = result
            kwargs['target'] = function
        super(ThreadWithResult, self).__init__(*args, **kwargs)

class Pipeline(object):
    class Popen(object):
        def __init__(self, command):
            self.command = command.split() if isinstance(command, str) else command
            self.process = None
        def start(self, prev, next):
            if prev:
                if prev.stdout:
                    stdin = prev.stdout
                else:
                    stdin = subprocess.PIPE
            else:
                stdin = None
            self.process = subprocess.Popen(self.command, stdin=stdin, stdout=subprocess.PIPE)
        def wait(self):
            if self.process: self.process.wait()
        def iter_with_prev(self, prev):
            if self.process:
                for line in self.process.stdout:
                    yield line
        def read_with_prev(self, prev, *args):
            if self.process: return self.process.stdout.read(*args)
        def readline_with_prev(self, prev, *args):
            if self.process: return self.process.stdout.readline(*args)
        def should_start_before_next(self, next): return True
        def should_start_after_next(self, next): return False
        uses_pipes = property(lambda self : True)
        error = property(lambda self : None)
        stdin = property(lambda self : (self.process.stdin if self.process else None))
        stdout = property(lambda self : (self.process.stdout if self.process else None))
        def __str__(self): return ' '.join(self.command)

    class IteratorReader(object):
        def __init__(self, iterator):
            self.iterator = iterator
            self.remaining = ''
        def read(self, *args): return self._read(False, *args)
        def readline(self, *args): return self._read(True, *args)
        def readall(self):
            result = self.remaining + '\n'.join(self.iterator)
            self.remaining = ''
            return result
        def _read(self, oneline, *args):
            size = args[0] if args else -1
            if size == 0: return ''
            if size < 0: return self.readall()
            result = self.remaining
            while len(result) < size:
                data = self.iterator.__next__()
                if data:
                    result += data + '\n'
                    if oneline: break
                else: break
            if len(result) >= size:
                self.remaining = result[size:]
                return result[:size]
            else:
                self.remaining = ''
                return result

    class FilterOutput(object):
        def __init__(self, target):
            self.target = target
            self.thread = None
            self.reader = None
            self.error = None
        def start(self, prev, next):
            if next and next.stdin:
                stdin = prev.stdout or prev
                stdout = next.stdin
                self.thread = threading.Thread(target=self.pipe_into, args=(stdin, stdout))
                self.thread.daemon = True
                self.thread.start()
        def pipe_into(self, stdin, stdout):
            try:
                for output in self.target(stdin):
                    stdout.write(output)
            except IOError:
                self.error = traceback.format_exc()
            finally:
                stdout.close()
        def wait(self):
            if self.thread: self.thread.join()
        def iter_with_prev(self, prev):
            for line in self.target(prev.stdout or prev):
                yield line.rstrip('\n\r')
        def read_with_prev(self, prev, *args):
            if not self.reader: self.reader = Pipeline.IteratorReader(self.target(prev))
            return self.reader.read(*args)
        def readline_with_prev(self, prev, *args):
            if not self.reader: self.reader = Pipeline.IteratorReader(self.target(prev))
            return self.reader.readline(*args)
        def should_start_before_next(self, next): return next and not next.uses_pipes
        def should_start_after_next(self, next): return next and next.uses_pipes
        uses_pipes = property(lambda self : False)
        stdin = property(lambda self : None)
        stdout = property(lambda self : None)
        def __str__(self): return self.target.__name__

    def __init__(self, plugin, parent):
        self.plugin = plugin
        self.parent = parent
    @staticmethod
    def Process(command, parent=None):
        return Pipeline(Pipeline.Popen(command), parent)
    @staticmethod
    def Filter(target, parent=None):
        return Pipeline(Pipeline.FilterOutput(target), parent)
    def pipe_into(self, command):
        return Pipeline.Process(command, self)
    def pipe_into_pipeline(self, pipeline):
        pipeline.root.parent = self
        return pipeline
    def filter_output(self, target):
        return Pipeline.Filter(target, self)
    def will_start_next(self, next):
        if self.plugin.should_start_before_next(next): self.start(next)
    def did_start_next(self, next):
        if self.plugin.should_start_after_next(next): self.start(next)
    def start(self, next=None):
        if self.parent: self.parent.will_start_next(self)
        self.plugin.start(self.parent, next)
        if self.parent: self.parent.did_start_next(self)
        return self
    def wait(self):
        for pipeline in self.pipelines: pipeline.plugin.wait()
    def get_root(self):
        for pipeline in self.pipelines: root = pipeline
        return root
    def get_pipelines(self):
        pipeline = self
        while pipeline:
            yield pipeline
            pipeline = pipeline.parent
    root = property(lambda self : self.get_root())
    pipelines = property(lambda self : self.get_pipelines())
    uses_pipes = property(lambda self : self.plugin.uses_pipes)
    stdin = property(lambda self : self.plugin.stdin)
    stdout = property(lambda self : self.plugin.stdout)
    error = property(lambda self : next((pipeline.plugin.error for pipeline in self.pipelines if pipeline.plugin.error), None))
    def __str__(self): return '\n'.join(reversed([str(pipeline.plugin) for pipeline in self.pipelines]))
    def __iter__(self): return self.plugin.iter_with_prev(self.parent)
    def read(self, *args): return self.plugin.read_with_prev(self.parent, *args)
    def readline(self, *args): return self.plugin.readline_with_prev(self.parent, *args)

class GitFat(object):
    DecodeError = RuntimeError
    def __init__(self, verbose=False):
        self.verbose = verbose_stderr if verbose else verbose_ignore
        try:
            self.gitroot = subprocess.check_output('git rev-parse --show-toplevel'.split()).strip()
        except subprocess.CalledProcessError:
            sys.exit(1)
        self.gitdir = subprocess.check_output('git rev-parse --git-dir'.split()).strip()
        self.objdir = os.path.join(self.gitdir, 'fat', 'objects')
        if os.environ.get('GIT_FAT_VERSION') == '1':
            self.encode = self.encode_v1
        else:
            self.encode = self.encode_v2
        def magiclen(enc):
            return len(enc(hashlib.sha1('dummy').hexdigest(), 5))
        self.magiclen = magiclen(self.encode) # Current version
        self.magiclens = [magiclen(enc) for enc in [self.encode_v1, self.encode_v2]] # All prior versions
    def setup(self):
        mkdir_p(self.objdir)
    def is_init_done(self):
        return gitconfig_get('filter.fat.clean') or gitconfig_get('filter.fat.smudge')
    def is_init_done_v2(self):
        return self.is_init_done() and '%f' in gitconfig_get('filter.fat.clean')
    def assert_init_done(self):
        if not self.is_init_done():
            sys.stderr.write('fatal: git-fat is not yet configured in this repository.\n')
            sys.stderr.write('Run "git fat init" to configure.\n')
            sys.exit(1)
    def get_rsync(self):
        cfgpath   = os.path.join(self.gitroot,'.gitfat')
        remote    = gitconfig_get('rsync.remote', file=cfgpath)
        ssh_port  = gitconfig_get('rsync.sshport', file=cfgpath)
        ssh_user  = gitconfig_get('rsync.sshuser', file=cfgpath)
        options   = gitconfig_get('rsync.options', file=cfgpath)
        if remote is None:
            raise RuntimeError('No rsync.remote in %s' % cfgpath)
        return remote, ssh_port, ssh_user, options
    def get_rsync_command(self,push):
        (remote, ssh_port, ssh_user, options) = self.get_rsync()
        if push:
            self.verbose('Pushing to %s' % (remote))
        else:
            self.verbose('Pulling from %s' % (remote))

        cmd = ['rsync', '--progress', '--ignore-existing', '--from0', '--files-from=-']
        rshopts = ''
        if ssh_user:
            rshopts += ' -l ' + ssh_user
        if ssh_port:
            rshopts += ' -p ' + ssh_port
        if rshopts:
            cmd.append('--rsh=ssh' + rshopts)
        if options:
            cmd += options.split(' ')
        if push:
            cmd += [self.objdir + '/', remote + '/']
        else:
            cmd += [remote + '/', self.objdir + '/']
        return cmd
    def file_hash(self, fname):
        try:
            h = hashlib.new('sha1')
            for block in readblocks(open(fname)):
                h.update(block)
            return h.hexdigest()
        except OSError:
            return None
    def revparse(self, revname):
        return subprocess.check_output(['git', 'rev-parse', revname]).strip()
    def encode_v1(self, digest, bytes):
        'Produce legacy representation of file to be stored in repository.'
        return '#$# git-fat %s\n' % (digest,)
    def encode_v2(self, digest, bytes):
        'Produce representation of file to be stored in repository. 20 characters can hold 64-bit integers.'
        return '#$# git-fat %s %20d\n' % (digest, bytes)
    def decode(self, string, noraise=False):
        cookie = '#$# git-fat '
        if string.startswith(cookie):
            parts = string[len(cookie):].split()
            digest = parts[0]
            bytes = int(parts[1]) if len(parts) > 1 else None
            return digest, bytes
        elif noraise:
            return None, None
        else:
            raise GitFat.DecodeError('Could not decode %s' % (string))
    def decode_stream(self, stream):
        'Return digest if git-fat cache, otherwise return iterator over entire file contents'
        preamble = stream.read(self.magiclen)
        try:
            return self.decode(preamble)
        except GitFat.DecodeError:
            # Not sure if this is the right behavior
            return itertools.chain([preamble], readblocks(stream)), None
    def decode_file(self, fname):
        # Fast check
        try:
            stat = os.lstat(fname)
        except OSError:
            return False, None, 0
        filesize = stat.st_size
        if filesize != self.magiclen:
            return False, None, filesize
        # read file
        try:
            digest, bytes = self.decode_stream(open(fname))
        except IOError:
            return False, None, filesize
        if isinstance(digest, str):
            return digest, bytes, filesize
        else:
            return None, bytes, filesize
    def decode_index_file(self, filename):
        filesize = 0
        metadata = subprocess.check_output(['git', 'ls-files', '-s', filename])
        if metadata:
            objhash = metadata.split(None, 2)[1]
            filesize = int(subprocess.check_output(['git', 'cat-file', '-s', objhash]))
            if filesize == self.magiclen:
                content = subprocess.check_output(['git', 'cat-file', '-p', objhash])
                try:
                    digest, bytes = self.decode(content)
                    return digest, bytes, filesize
                except GitFat.DecodeError:
                    pass
        return False, None, filesize
    def decode_clean(self, body):
        '''
        Attempt to decode version in working tree. The tree version could be changed to have a more
        useful message than the machine-readable copy that goes into the repository. If the tree
        version decodes successfully, it indicates that the fat data is not currently available in
        this repository.
        '''
        digest, bytes = self.decode(body, noraise=True)
        return digest
    def filter_clean(self, instream, outstreamclean, filename=None):
        h = hashlib.new('sha1')
        bytes = 0
        fd, tmpname = tempfile.mkstemp(dir=self.objdir)
        suffix_for_verbose = ' %s' % filename if filename else ''
        try:
            ishanging = False
            cached = False                # changes to True when file is cached
            with os.fdopen(fd, 'w') as cache:
                outstream = cache
                firstblock = True
                for block in readblocks(instream):
                    if firstblock:
                        if len(block) == self.magiclen and self.decode_clean(block[0:self.magiclen]):
                            ishanging = True              # Working tree version is verbatim from repository (not smudged)
                            outstream = outstreamclean
                        elif filename:
                            # If previously added without using fat, don't try to convert it to fat now
                            digest, _, filesize = self.decode_index_file(filename)
                            if not digest and filesize > 0:
                                self.verbose('git-fat filter-clean: skipping because previously added as non-fat file: %s' % filename)
                                ishanging = True
                                outstream = outstreamclean
                        firstblock = False
                    h.update(block)
                    bytes += len(block)
                    outstream.write(block)
                outstream.flush()
            digest = h.hexdigest()
            objfile = os.path.join(self.objdir, digest)
            if not ishanging:
                if os.path.exists(objfile):
                    self.verbose('git-fat filter-clean: cache already exists %s%s' % (objfile, suffix_for_verbose))
                    os.remove(tmpname)
                else:
                    # Set permissions for the new file using the current umask
                    os.chmod(tmpname, int('444', 8) & ~umask())
                    os.rename(tmpname, objfile)
                    self.verbose('git-fat filter-clean: caching to %s%s' % (objfile, suffix_for_verbose))
                cached = True
                outstreamclean.write(self.encode(digest, bytes))
        finally:
            if not cached:
                os.remove(tmpname)

    def cmd_filter_clean(self, filename=None):
        '''
        The clean filter runs when a file is added to the index. It gets the "smudged" (tree)
        version of the file on stdin and produces the "clean" (repository) version on stdout.
        '''
        self.setup()
        self.filter_clean(sys.stdin, sys.stdout, filename)
    def cmd_filter_smudge(self, filename=None):
        self.setup()
        result, bytes = self.decode_stream(sys.stdin)
        suffix_for_verbose = ' %s' % filename if filename else ''
        if isinstance(result, str):       # We got a digest
            objfile = os.path.join(self.objdir, result)
            try:
                cat(open(objfile), sys.stdout)
                self.verbose('git-fat filter-smudge: restoring from %s%s' % (objfile, suffix_for_verbose))
            except IOError:                 # file not found
                self.verbose('git-fat filter-smudge: fat object missing %s%s' % (objfile, suffix_for_verbose))
                sys.stdout.write(self.encode(result, bytes))   # could leave a better notice about how to recover this file
        else:                             # We have an iterable over the original input.
            self.verbose('git-fat filter-smudge: not a managed file%s' % suffix_for_verbose)
            cat_iter(result, sys.stdout)
    def catalog_objects(self):
        try:
            return set(os.listdir(self.objdir)) # Dir may not exist
        except OSError:
            return set()
    def referenced_objects(self, rev=None, all=False):
        get_referenced = self.referenced_objects_pipeline(rev, all).start()
        referenced = set([line.split('\t', 1)[0] for line in get_referenced])
        get_referenced.wait()
        if get_referenced.error: self.verbose(get_referenced.error)
        return referenced
    def referenced_objects_pipeline(self, rev=None, all=False, with_filenames=False):
        return self.objects_pipeline(rev, all, referenced_only=True, with_filenames=with_filenames)
    def objects_pipeline(self, rev=None, all=False, referenced_only=False, with_filenames=False):
        if all:
            rev = '--all'
        elif rev is None:
            rev = self.revparse('HEAD')

        # Revision list gives us objects to inspect with cat-file
        return Pipeline.Process(['git','rev-list','--objects',rev]) \
            .pipe_into_pipeline(self.objects_from_revlist_pipeline(referenced_only, with_filenames))
    def objects_from_revlist_pipeline(self, referenced_only=False, with_filenames=False):
        with_fileinfos = with_filenames or not referenced_only
        if with_filenames: filenames_queue = Queue.Queue()
        if with_fileinfos: fileinfos_queue = Queue.Queue()

        def cut_sha1hash(input):
            for line in input:
                parts = line.split(None, 1) # objhash, filename_with_spaces_and_newline
                objhash = parts[0]
                if with_filenames:
                    filename = parts[1].rstrip() if len(parts) > 1 else ''
                    filenames_queue.put((objhash, filename))
                yield objhash + '\n'
        def filter_gitfat_candidates(input):
            for line in input:
                parts = line.split() # objhash, objtype, size, \n
                if len(parts) < 3:
                    # Invalid file
                    if with_filenames: filenames_queue.get_nowait() # Discard
                else:
                    objhash, objtype, size = parts
                    if with_fileinfos:
                        queued_obj, queued_filename = filenames_queue.get_nowait() if with_filenames else ('','')
                        fileinfos_queue.put((objhash, size, queued_filename))
                    if objtype == 'blob' and int(size) in self.magiclens:
                        yield objhash + '\n'
        def extract_fathashes_from_gitfat_refs(input):
            UNREFERENCED = '\t0\t%s\t%s\t%s\n'
            REFERENCED = '%s\t%s\t%s\t%s\t%s\n'

            # Process metadata + content format provided by `cat-file --batch`
            while True:
                metadata_line = input.readline()
                if not metadata_line:
                    break  # EOF
                objhash, objtype, size_str = metadata_line.split()
                filename = ''
                if with_fileinfos:
                    # Get filename if matches referenced, else flush all unreferenced
                    while True:
                        queued_obj, queued_size, queued_filename = fileinfos_queue.get_nowait()
                        if queued_obj == objhash:
                            filename = queued_filename
                            break
                        elif not referenced_only:
                            yield UNREFERENCED % (queued_obj, queued_size, queued_filename)

                size, bytes_read = int(size_str), 0
                # We know from filter that item is a candidate git-fat object and
                # is small enough to read into memory and process
                content = ''
                while bytes_read < size:
                    data = input.read(size - bytes_read)
                    if not data:
                        break  # EOF
                    content += data
                    bytes_read += len(data)
                try:
                    fathash, fatbytes = self.decode(content)
                    yield REFERENCED % (fathash, fatbytes, objhash, size_str, filename)
                except GitFat.DecodeError:
                    self.verbose('decode error: %s' % objhash)
                    if not referenced_only:
                        yield UNREFERENCED % (objhash, size_str, filename)
                    else:
                        pass
                # Consume LF record delimiter in `cat-file --batch` output
                bytes_read = 0
                while bytes_read < 1:
                    data = input.read(1)
                    if not data:
                        break  # EOF
                    bytes_read += len(data)
            # Yield remaining unreferenced fileinfos
            if with_fileinfos:
                while not fileinfos_queue.empty():
                    objhash, size, filename = fileinfos_queue.get()
                    if not referenced_only:
                        yield UNREFERENCED % (objhash, size, filename)

        # 1. Cut hash from revlist input
        # 2. Get info for git-fat candidates in bulk
        # 3. Analyse info to identify git-fat candidates
        # 4. Get full contents of identified git-fat candidates in bulk
        # 5. Extract fathash using file metadata and contents
        return Pipeline.Filter(cut_sha1hash) \
            .pipe_into('git cat-file --batch-check') \
            .filter_output(filter_gitfat_candidates) \
            .pipe_into('git cat-file --batch') \
            .filter_output(extract_fathashes_from_gitfat_refs)

    def orphan_files(self, patterns=[]):
        'generator for all orphan placeholders in the working tree'
        if not patterns or patterns == ['']:
            patterns = ['.']
        for fname in subprocess.check_output(['git', 'ls-files', '-z'] + patterns).split('\x00')[:-1]:
            digest = self.decode_file(fname)[0]
            if digest:
                yield (digest, fname)

    def cmd_status(self, args):
        if '--all' in args:
            self.status_no_paths(all=True)
        elif args:
            self.status_for_paths(args)
        else:
            self.status_no_paths(all=False)
    def status_no_paths(self, all=False):
        def print_fat_with_filename(fat, indent=0, obj=None, filename=None):
            indent_str = ' ' * indent if indent > 0 else ''
            filename_str = filename if filename is not None else 'obj=%s' % obj if obj else 'file not found'
            print('%s%s%s' % (indent_str, fat, (' %s' % filename_str) if filename_str else ''))
        def did_get_metadata_line(line):
            fat, fatsize, obj, objsize, filename = line.split('\t', 4)
            if all:
                print_fat_with_filename(fat, obj=obj, filename=filename.rstrip())
            return (fat, filename)
        load_catalog = ThreadWithResult(name='catalog', target=lambda : self.catalog_objects())
        load_catalog.daemon = True
        load_catalog.start()
        load_referenced = self.referenced_objects_pipeline(all=all, with_filenames=all).start()
        load_fat2file = ThreadWithResult(name='referenced', target=lambda : dict([did_get_metadata_line(line) for line in load_referenced]))
        load_fat2file.daemon = True
        load_fat2file.start()
        load_referenced.wait()
        load_fat2file.join()
        load_catalog.join()
        if load_referenced.error: self.verbose(load_referenced.error)
        fat2file = load_fat2file.result
        referenced = set(fat2file.keys())
        catalog = load_catalog.result
        orphans = referenced - catalog
        garbages = catalog - referenced
        if orphans:
            print('Orphan objects:')
            for orphan in orphans:
                print_fat_with_filename(orphan, indent=6, filename=fat2file[orphan])
        if garbages:
            print('Garbage objects:')
            for garbage in garbages:
                print_fat_with_filename(garbage, indent=6, filename='')
    def status_for_paths(self, paths):
        def having_fat_filter(input):
            # Input format = [mode] [obj] [stage] [filename] \x00 filter \x00 fat \x00
            part_index = 0
            filename = obj = None
            for line in input:
                yield 'francis sucks\n'
                for part in line.split('\x00'):
                    if not part:
                        break; # EOF
                    if part_index == 0:
                        mode, obj, stage, filename = part.split(None, 3)
                        if mode.startswith('100'):
                            filename = filename.rstrip()
                        else:
                            filename = obj = None
                    elif part_index == 1:
                        if filename and part != 'filter':
                            filename = obj = None
                    elif part_index == 2:
                        if filename and part == 'fat':
                            yield '%s %s\n' % (obj, filename)
                    part_index = (part_index + 1) % 3
        def did_get_metadata_line(line):
            fat, fatsize, obj, objsize, filename = line.split('\t')
            if not filename: return
            if fat:
                fatfile = os.path.join(self.objdir, fat)
                fatstatus = FatStatus.OK if os.path.exists(fatfile) else FatStatus.MISSING
                fat_from_file, fatsize_from_file, filesize = self.decode_file(filename)
                if fat_from_file:
                    filestatus = FileStatus.ORPHAN
                else:
                    if filesize != int(fatsize):
                        filestatus = FileStatus.DIFF
                    else:
                        filestatus = FileStatus.OK
                print_fat(fat, filename, fatstatus, filestatus)
            else:
                filehash = self.file_hash(filename)
                if not filehash or not os.path.exists(os.path.join(self.objdir, filehash)):
                    print_fat(filehash, filename, FatStatus.NOTFAT, FileStatus.NOTFAT)
                else:
                    print_fat(filehash, filename, FatStatus.GARBAGE, FileStatus.NOTFAT)
        def print_fat(hash, fname, fatstatus, filestatus):
            if fatstatus == FatStatus.MISSING:
                if filestatus == FileStatus.DIFF:
                    status = PrintStatus.ERROR
                else:
                    status = PrintStatus.WARN
            elif filestatus == FileStatus.ORPHAN:
                if fatstatus == FatStatus.MISSING:
                    status = PrintStatus.ERROR
                else:
                    status = PrintStatus.WARN
            elif fatstatus == FatStatus.GARBAGE:
                status = PrintStatus.WARN2
            elif filestatus == FileStatus.NOTFAT:
                status = PrintStatus.WARN3
            else:
                status = PrintStatus.DEFAULT
            if status.is_error:
                status_str = 'x-%s-%s' % (str(filestatus).center(4, '-'), str(fatstatus).center(4, '-'))
            else:
                status_str = '-' * 11
            print_status(status, '%s %s %s' % (status_str, hash, fname))

        paths = [os.path.relpath(os.path.abspath(path), '.') for path in paths]
        get_metadata = Pipeline.Process(['git','ls-files','-s','-z'] + list(paths)) \
            .pipe_into('git check-attr --stdin -z filter') \
            .filter_output(having_fat_filter) \
            .pipe_into_pipeline(self.objects_from_revlist_pipeline(referenced_only=False, with_filenames=True)) \
            .start()
        for line in get_metadata:
            did_get_metadata_line(line)
        get_metadata.wait()
        if get_metadata.error: self.verbose(get_metadata.error)
    def is_dirty(self):
        return subprocess.call(['git', 'diff-index', '--quiet', 'HEAD']) == 0
    def cmd_push(self, args):
        'Push anything that I have stored and referenced'
        self.setup()
        # Default to push only those objects referenced by current HEAD
        # (includes history). Finer-grained pushing would be useful.
        pushall = '--all' in args
        files = self.referenced_objects(all=pushall) & self.catalog_objects()
        cmd = self.get_rsync_command(push=True)
        self.verbose('Executing: %s' % ' '.join(cmd))
        p = subprocess.Popen(cmd, stdin=subprocess.PIPE)
        p.communicate(input='\x00'.join(files))
        if p.returncode:
            sys.exit(p.returncode)
    def checkout(self, files=None, pull_if_needed=False):
        'Update any stale files in the present working tree'
        self.assert_init_done()
        orphan_files = self.orphan_files(files)
        if pull_if_needed:
            orphan_files = list(orphan_files)
            orphan = set([orphan_file[0] for orphan_file in orphan_files])
            catalog = self.catalog_objects()
            self.pull(orphan - catalog)
        for digest, fname in orphan_files:
            objpath = os.path.join(self.objdir, digest)
            if os.access(objpath, os.R_OK):
                print('Restoring %s -> %s' % (digest, fname))
                # The output of our smudge filter depends on the existence of
                # the file in .git/fat/objects, but git caches the file stat
                # from the previous time the file was smudged, therefore it
                # won't try to re-smudge. I don't know a git command that
                # specifically invalidates that cache, but changing the mtime
                # on the file will invalidate the cache.
                # Here we set the mtime to mtime + 1. This is an improvement
                # over touching the file as it catches the edgecase where a
                # git-checkout happens within the same second as a git fat
                # checkout.
                stat = os.lstat(fname)
                os.utime(fname, (stat.st_atime, stat.st_mtime + 1))
                # This re-smudge is essentially a copy that restores
                # permissions.
                subprocess.check_call(
                    ['git', 'checkout-index', '--index', '--force', fname])
            elif pull_if_needed:
                print('Data unavailable: %s %s' % (digest,fname))
    def cmd_pull(self, args):
        'Pull anything that I have referenced, but not stored'
        self.setup()
        refargs = dict()
        if '--all' in args:
            refargs['all'] = True
        for arg in args:
            if arg.startswith('-') or len(arg) != 40:
                continue
            rev = self.revparse(arg)
            if rev:
                refargs['rev'] = rev
        files = self.filter_objects(refargs, self.parse_pull_patterns(args))
        self.pull(files)
        self.checkout(files)
    def parse_pull_patterns(self, args):
        if '--' not in args:
            return ['']
        else:
            idx = args.index('--')
            patterns  = args[idx+1:] #we don't care about '--'
            return patterns
    def pull(self, files):
        cmd = self.get_rsync_command(push=False)
        self.verbose('Executing: %s' % ' '.join(cmd))
        p = subprocess.Popen(cmd, stdin=subprocess.PIPE)
        p.communicate(input='\x00'.join(files))
        if p.returncode:
            sys.exit(p.returncode)

    def filter_objects(self, refargs, patterns):
        files = self.referenced_objects(**refargs) - self.catalog_objects()
        if refargs.get('all'): # Currently ignores patterns; can we efficiently do both?
            return files
        orphans_matched = list(self.orphan_files(patterns))
        orphans_objects = set(map(lambda x: x[0], orphans_matched))
        return files & orphans_objects

    def cmd_checkout(self, args):
        self.checkout(pull_if_needed=True)

    def cmd_gc(self):
        garbage = self.catalog_objects() - self.referenced_objects()
        print('Unreferenced objects to remove: %d' % len(garbage))
        for obj in garbage:
            fname = os.path.join(self.objdir, obj)
            print('%10d %s' % (os.stat(fname).st_size, obj))
            os.remove(fname)

    def cmd_verify(self):
        """Print details of git-fat objects with incorrect data hash"""
        corrupted_objects = []
        for obj in self.catalog_objects():
            fname = os.path.join(self.objdir, obj)
            data_hash = self.file_hash(fname)
            if obj != data_hash:
                corrupted_objects.append((obj, data_hash))
        if corrupted_objects:
            print('Corrupted objects: %d' % len(corrupted_objects))
            for obj, data_hash in corrupted_objects:
                print('%s data hash is %s' % (obj, data_hash))
            sys.exit(1)

    def cmd_init(self):
        self.setup()
        if self.is_init_done_v2():
            print('Git fat already configured, check configuration in .git/config')
        else:
            gitconfig_set('filter.fat.clean', 'git-fat filter-clean %f')
            gitconfig_set('filter.fat.smudge', 'git-fat filter-smudge %f')
            print('Initialized git fat')
    def gen_large_blobs(self, revs, threshsize):
        """Build dict of all blobs"""
        time0 = time.time()
        def hash_only(input, output):
            """The output of git rev-list --objects shows extra info for blobs, subdirectory trees, and tags.
            This truncates to one hash per line.
            """
            for line in input:
                output.write(line[:40] + '\n')
            output.close()
        revlist = subprocess.Popen(['git', 'rev-list', '--all', '--objects'], stdout=subprocess.PIPE, bufsize=-1)
        objcheck = subprocess.Popen(['git', 'cat-file', '--batch-check'], stdin=subprocess.PIPE, stdout=subprocess.PIPE, bufsize=-1)
        hashonly = threading.Thread(target=hash_only, args=(revlist.stdout, objcheck.stdin))
        hashonly.start()
        numblobs = 0; numlarge = 1
        # Build dict with the sizes of all large blobs
        for line in objcheck.stdout:
            objhash, blob, size = line.split()
            if blob != 'blob':
                continue
            size = int(size)
            numblobs += 1
            if size > threshsize:
                numlarge += 1
                yield objhash, size
        revlist.wait()
        objcheck.wait()
        hashonly.join()
        time1 = time.time()
        self.verbose('%d of %d blobs are >= %d bytes [elapsed %.3fs]' % (numlarge, numblobs, threshsize, time1-time0))
    def cmd_find(self, args):
        maxsize = int(args[0])
        blobsizes = dict(self.gen_large_blobs('--all', maxsize))
        time0 = time.time()
        # Find all names assumed by large blobs (those in blobsizes)
        pathsizes = collections.defaultdict(lambda:set())
        difftree = Pipeline.Process(['git', 'rev-list', '--all']) \
            .pipe_into(['git', 'diff-tree', '--root', '--no-renames', '--no-commit-id', '--diff-filter=AMCR', '-r', '--stdin', '-z']) \
            .start()
        for newblob, modflag, path in difftreez_reader(difftree.stdout):
            bsize = blobsizes.get(newblob)
            if bsize:                     # We care about this blob
                pathsizes[path].add(bsize)
        time1 = time.time()
        self.verbose('Found %d paths in %.3f s' % (len(pathsizes), time1-time0))
        maxlen = max(map(len,pathsizes)) if pathsizes else 0
        for path, sizes in sorted(pathsizes.items(), key=lambda ps: max(ps[1]), reverse=True):
            print('%-*s filter=fat -text # %10d %d' % (maxlen, path,max(sizes),len(sizes)))
        difftree.wait()
    def cmd_index_filter(self, args):
        manage_gitattributes = '--manage-gitattributes' in args
        filelist = set(f.strip() for f in open(args[0]).readlines())
        lsfiles = subprocess.Popen(['git', 'ls-files', '-s'], stdout=subprocess.PIPE)
        updateindex = subprocess.Popen(['git', 'update-index', '--index-info'], stdin=subprocess.PIPE)
        for line in lsfiles.stdout:
            mode, sep, tail = line.partition(' ')
            blobhash, sep, tail = tail.partition(' ')
            stageno, sep, tail = tail.partition('\t')
            filename = tail.strip()
            if filename not in filelist:
                continue
            if mode == "120000":
                # skip symbolic links
                continue
            # This file will contain the hash of the cleaned object
            hashfile = os.path.join(self.gitdir, 'fat', 'index-filter', blobhash)
            try:
                cleanedobj = open(hashfile).read().rstrip()
            except IOError:
                catfile = subprocess.Popen(['git', 'cat-file', 'blob', blobhash], stdout=subprocess.PIPE)
                hashobject = subprocess.Popen(['git', 'hash-object', '-w', '--stdin'], stdin=subprocess.PIPE, stdout=subprocess.PIPE)
                def dofilter():
                    self.filter_clean(catfile.stdout, hashobject.stdin, filename)
                    hashobject.stdin.close()
                filterclean = threading.Thread(target=dofilter)
                filterclean.start()
                cleanedobj = hashobject.stdout.read().rstrip()
                catfile.wait()
                hashobject.wait()
                filterclean.join()
                mkdir_p(os.path.dirname(hashfile))
                open(hashfile, 'w').write(cleanedobj + '\n')
            updateindex.stdin.write('%s %s %s\t%s\n' % (mode, cleanedobj, stageno, filename))
        if manage_gitattributes:
            try:
                mode, blobsha1, stageno, filename = subprocess.check_output(['git', 'ls-files', '-s', '.gitattributes']).split()
                gitattributes_lines = subprocess.check_output(['git', 'cat-file', 'blob', blobsha1]).splitlines()
            except ValueError:  # Nothing to unpack, thus no file
                mode, stageno = '100644', '0'
                gitattributes_lines = []
            gitattributes_extra = ['%s filter=fat -text' % line.split()[0] for line in filelist]
            hashobject = subprocess.Popen(['git', 'hash-object', '-w', '--stdin'], stdin=subprocess.PIPE, stdout=subprocess.PIPE)
            stdout, stderr = hashobject.communicate('\n'.join(gitattributes_lines + gitattributes_extra) + '\n')
            updateindex.stdin.write('%s %s %s\t%s\n' % (mode, stdout.strip(), stageno, '.gitattributes'))
        updateindex.stdin.close()
        lsfiles.wait()
        updateindex.wait()

def main(fat, cmd, args):
    if cmd == 'filter-clean':
        fat.cmd_filter_clean(args[0] if args else None)
    elif cmd == 'filter-smudge':
        fat.cmd_filter_smudge(args[0] if args else None)
    elif cmd == 'init':
        fat.cmd_init()
    elif cmd == 'status':
        fat.cmd_status(args)
    elif cmd == 'push':
        fat.cmd_push(args)
    elif cmd == 'pull':
        fat.cmd_pull(args)
    elif cmd == 'gc':
        fat.cmd_gc()
    elif cmd == 'verify':
        fat.cmd_verify()
    elif cmd == 'checkout':
        fat.cmd_checkout(args)
    elif cmd == 'find':
        fat.cmd_find(*args)
    elif cmd == 'index-filter':
        fat.cmd_index_filter(args)
    else:
        print('Usage: git fat [init|status|push|pull|gc|verify|checkout|find|index-filter] [--verbose]', file=sys.stderr)

if __name__ == '__main__':
    cmd, verbose, args = (sys.argv[1], '--verbose' in sys.argv, [arg for arg in sys.argv[2:] if arg != '--verbose']) if len(sys.argv) > 1 else ('', False, [])
    if not verbose:
        verbose = os.environ.get('GIT_FAT_VERBOSE')
    fat = GitFat(verbose=verbose)
    if verbose:
        main(fat, cmd, args)
    else:
        try:
            main(fat, cmd, args)
        except KeyboardInterrupt:
            sys.exit()
        except Exception as e:
            prefix = '[git-fat] ' if cmd == 'filter-clean' or cmd == 'filter-smudge' else ''
            print('%s%s' % (prefix, e.message or 'An error occurred with type %s' % type(e).__name__), file=sys.stderr)
            sys.exit(1)
